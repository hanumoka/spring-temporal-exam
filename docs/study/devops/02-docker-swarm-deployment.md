# Docker Swarm 배포 가이드

> **목표**: Docker Swarm을 활용한 Temporal + MSA 프로덕션 배포 전략 이해
> **작성일**: 2026-02-05

---

## 1. Docker Swarm 개요

### What (무엇인가)

Docker Swarm은 Docker 엔진에 내장된 컨테이너 오케스트레이션 도구입니다.

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Docker Swarm = Docker 내장 오케스트레이션                                  │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  핵심 기능                                                           │   │
│  ├─────────────────────────────────────────────────────────────────────┤   │
│  │                                                                      │   │
│  │  ✅ 서비스 디스커버리    - 내장 DNS로 서비스 이름 해석              │   │
│  │  ✅ 로드 밸런싱          - VIP 기반 자동 분산                        │   │
│  │  ✅ 롤링 업데이트        - 무중단 배포                               │   │
│  │  ✅ 자동 복구            - 컨테이너 실패 시 재시작                   │   │
│  │  ✅ 시크릿 관리          - 암호화된 민감 정보 저장                   │   │
│  │  ✅ 설정 관리            - Config 객체로 설정 분리                   │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Why (왜 필요한가)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Kubernetes vs Docker Swarm 비교                                            │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  항목              │  Kubernetes          │  Docker Swarm                   │
│  ─────────────────────────────────────────────────────────────────────────  │
│  러닝커브          │  높음 (수개월)       │  낮음 (수일)                    │
│  설정 복잡도       │  복잡 (CRD, RBAC)    │  단순 (docker-compose 확장)     │
│  운영 비용         │  높음 (전담 인력)    │  낮음 (개발자가 직접 가능)      │
│  기능              │  매우 풍부           │  기본 기능 충분                 │
│  에코시스템        │  거대함              │  제한적                         │
│  오토스케일링      │  ✅ HPA             │  ❌ 수동 (docker service scale) │
│  서비스 메시       │  ✅ Istio 등        │  ❌ 없음                        │
│  적합한 규모       │  대규모 (50+ 서비스) │  중소규모 (10-30 서비스)        │
│                                                                             │
│  결론: 현재 프로젝트 규모 (5-6개 서비스)에는 Docker Swarm이 적합            │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 참고 자료

- [Docker Swarm mode 공식 문서](https://docs.docker.com/engine/swarm/)
- [Docker Swarm in 2025](https://medium.com/@niksa.makitan/docker-swarm-in-2025-0d2f2bc5d929)
- [Deploy to Swarm](https://docs.docker.com/guides/swarm-deploy/)

---

## 2. Swarm 클러스터 아키텍처

### 2.1 노드 역할

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│                        Docker Swarm 클러스터 구조                            │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                        Manager Nodes                                 │  │
│   │                                                                      │  │
│   │   역할:                                                              │  │
│   │   • 클러스터 상태 관리 (Raft 합의 알고리즘)                         │  │
│   │   • 서비스 스케줄링 (어떤 노드에 컨테이너 배치할지 결정)            │  │
│   │   • Swarm API 엔드포인트 제공                                       │  │
│   │   • 시크릿/설정 저장 (암호화된 Raft 로그)                           │  │
│   │                                                                      │  │
│   │   ┌───────────┐    ┌───────────┐    ┌───────────┐                   │  │
│   │   │ Manager 1 │◄──►│ Manager 2 │◄──►│ Manager 3 │  Raft 합의       │  │
│   │   │ (Leader)  │    │ (Follower)│    │ (Follower)│                   │  │
│   │   └─────┬─────┘    └─────┬─────┘    └─────┬─────┘                   │  │
│   │         │                │                │                          │  │
│   └─────────┼────────────────┼────────────────┼──────────────────────────┘  │
│             │                │                │                             │
│             │    Gossip Protocol (SWIM)       │                             │
│             │                │                │                             │
│   ┌─────────┼────────────────┼────────────────┼──────────────────────────┐  │
│   │         ▼                ▼                ▼                          │  │
│   │                        Worker Nodes                                  │  │
│   │                                                                      │  │
│   │   역할:                                                              │  │
│   │   • 컨테이너 실행 (실제 워크로드)                                   │  │
│   │   • Manager의 지시를 받아 태스크 수행                               │  │
│   │                                                                      │  │
│   │   ┌───────────┐    ┌───────────┐    ┌───────────┐                   │  │
│   │   │ Worker 1  │    │ Worker 2  │    │ Worker 3  │                   │  │
│   │   │ [Task]    │    │ [Task]    │    │ [Task]    │                   │  │
│   │   │ [Task]    │    │ [Task]    │    │ [Task]    │                   │  │
│   │   └───────────┘    └───────────┘    └───────────┘                   │  │
│   │                                                                      │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│   ※ Manager 노드도 Worker 역할 수행 가능 (기본값)                          │
│   ※ 프로덕션: Manager는 관리 전용 권장 (--availability drain)             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 2.2 Manager 노드 수 권장사항

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Manager 노드 수와 장애 허용                                                │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Raft 합의 알고리즘: 과반수(N/2 + 1) 이상이 살아있어야 클러스터 동작       │
│                                                                             │
│  ┌──────────────┬──────────────┬──────────────┬──────────────────────────┐ │
│  │ Manager 수   │ 과반수 필요   │ 허용 장애    │ 권장 상황                │ │
│  ├──────────────┼──────────────┼──────────────┼──────────────────────────┤ │
│  │ 1            │ 1            │ 0            │ 개발/테스트              │ │
│  │ 2            │ 2            │ 0 ⚠️         │ 권장하지 않음            │ │
│  │ 3            │ 2            │ 1            │ 소규모 프로덕션          │ │
│  │ 5            │ 3            │ 2            │ 중규모 프로덕션          │ │
│  │ 7            │ 4            │ 3            │ 대규모/미션 크리티컬     │ │
│  └──────────────┴──────────────┴──────────────┴──────────────────────────┘ │
│                                                                             │
│  ⚠️ 2대 구성은 1대 장애 시 과반수 미달로 클러스터 중단 (비권장)            │
│                                                                             │
│  권장: 항상 홀수 개 (1, 3, 5, 7)                                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 3. 배포 시나리오별 구성

### 3.1 시나리오 A: 단일 노드 (개발/소규모)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  시나리오 A: 단일 노드 배포                                                 │
│                                                                             │
│  적합한 상황:                                                               │
│  • 개발/테스트 환경                                                         │
│  • 트래픽이 적은 소규모 서비스                                              │
│  • 비용 최소화 필요                                                         │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌─────────────────────────────────────────────────────────────────────┐  │
│   │                        Node 1 (Manager + Worker)                     │  │
│   │                                                                      │  │
│   │   ┌─────────────────────────────────────────────────────────────┐   │  │
│   │   │  Temporal Infrastructure                                     │   │  │
│   │   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │   │  │
│   │   │  │ temporal     │  │ temporal-    │  │ temporal-ui  │       │   │  │
│   │   │  │ (auto-setup) │  │ postgresql   │  │              │       │   │  │
│   │   │  │ :21733       │  │ :21432       │  │ :21088       │       │   │  │
│   │   │  └──────────────┘  └──────────────┘  └──────────────┘       │   │  │
│   │   └─────────────────────────────────────────────────────────────┘   │  │
│   │                                                                      │  │
│   │   ┌─────────────────────────────────────────────────────────────┐   │  │
│   │   │  Application Services                                        │   │  │
│   │   │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐       │   │  │
│   │   │  │ orchestrator │  │ service-     │  │ service-     │       │   │  │
│   │   │  │ -temporal    │  │ order        │  │ inventory    │       │   │  │
│   │   │  │ :21081       │  │ :21082       │  │ :21083       │       │   │  │
│   │   │  └──────────────┘  └──────────────┘  └──────────────┘       │   │  │
│   │   │  ┌──────────────┐                                            │   │  │
│   │   │  │ service-     │                                            │   │  │
│   │   │  │ payment      │                                            │   │  │
│   │   │  │ :21084       │                                            │   │  │
│   │   │  └──────────────┘                                            │   │  │
│   │   └─────────────────────────────────────────────────────────────┘   │  │
│   │                                                                      │  │
│   │   ┌─────────────────────────────────────────────────────────────┐   │  │
│   │   │  Shared Infrastructure                                       │   │  │
│   │   │  ┌──────────────┐  ┌──────────────┐                          │   │  │
│   │   │  │ mysql        │  │ redis        │                          │   │  │
│   │   │  │ :21306       │  │ :21379       │                          │   │  │
│   │   │  └──────────────┘  └──────────────┘                          │   │  │
│   │   └─────────────────────────────────────────────────────────────┘   │  │
│   │                                                                      │  │
│   └─────────────────────────────────────────────────────────────────────┘  │
│                                                                             │
│  장점: 단순, 저비용                                                         │
│  단점: SPOF (Single Point of Failure), HA 없음                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 시나리오 B: 2노드 (Stateless HA, Stateful 단일)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  시나리오 B: 2노드 배포 (Stateless HA + Stateful 단일)                      │
│                                                                             │
│  적합한 상황:                                                               │
│  • 애플리케이션 HA는 필요하지만 DB HA는 포기 가능                           │
│  • 비용과 가용성의 타협                                                     │
│  • Temporal 상태 유실 시 재시작으로 복구 가능한 워크로드                    │
│                                                                             │
│  ⚠️ 주의: Swarm Manager 2대는 권장되지 않음                                │
│     → 1대를 Manager, 1대를 Worker로 구성하거나                             │
│     → 둘 다 Manager로 구성 시 1대 장애 = 클러스터 중단                     │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌────────────────────────────────┐   ┌────────────────────────────────┐  │
│   │      Node 1 (Manager)          │   │      Node 2 (Worker)           │  │
│   │                                │   │                                │  │
│   │  ┌──────────────────────────┐  │   │  ┌──────────────────────────┐  │  │
│   │  │  Stateful Services       │  │   │  │  Stateless Services      │  │  │
│   │  │  (replicas: 1)           │  │   │  │  (replicas: 2-3)         │  │  │
│   │  │                          │  │   │  │                          │  │  │
│   │  │  • temporal              │  │   │  │  • orchestrator ×2       │  │  │
│   │  │  • temporal-postgresql   │  │   │  │  • service-order ×2      │  │  │
│   │  │  • mysql                 │  │   │  │  • service-inventory ×2  │  │  │
│   │  │  • redis                 │  │   │  │  • service-payment ×2    │  │  │
│   │  │                          │  │   │  │                          │  │  │
│   │  └──────────────────────────┘  │   │  └──────────────────────────┘  │  │
│   │                                │   │                                │  │
│   │  placement:                    │   │  placement:                    │  │
│   │    constraints:                │   │    (기본 - 모든 노드 가능)     │  │
│   │      - node.role == manager    │   │                                │  │
│   │                                │   │                                │  │
│   └────────────────────────────────┘   └────────────────────────────────┘  │
│                                                                             │
│   로드 밸런싱:                                                              │
│   Worker → http://service-order:21082 → Swarm LB → order.1 또는 order.2   │
│                                                                             │
│   장점: 애플리케이션 HA, 수평 확장 가능                                     │
│   단점: DB/Temporal 장애 시 서비스 중단                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.3 시나리오 C: 3노드 이상 (Full HA)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  시나리오 C: 3노드 이상 (Full HA)                                           │
│                                                                             │
│  적합한 상황:                                                               │
│  • 프로덕션 환경                                                            │
│  • 고가용성 필수                                                            │
│  • 다운타임 최소화 필요                                                     │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ┌──────────────────┐  ┌──────────────────┐  ┌──────────────────┐          │
│  │   Node 1         │  │   Node 2         │  │   Node 3         │          │
│  │   (Manager)      │  │   (Manager)      │  │   (Manager)      │          │
│  │                  │  │                  │  │                  │          │
│  │  temporal-       │  │  temporal-       │  │  temporal-       │          │
│  │  frontend ×1     │  │  history ×2      │  │  matching ×2     │          │
│  │                  │  │                  │  │                  │          │
│  │  orchestrator ×1 │  │  orchestrator ×1 │  │  orchestrator ×1 │          │
│  │  service-order   │  │  service-order   │  │  service-order   │          │
│  │  service-inv     │  │  service-inv     │  │  service-inv     │          │
│  │  service-pay     │  │  service-pay     │  │  service-pay     │          │
│  │                  │  │                  │  │                  │          │
│  └────────┬─────────┘  └────────┬─────────┘  └────────┬─────────┘          │
│           │                     │                     │                     │
│           └─────────────────────┼─────────────────────┘                     │
│                                 │                                           │
│                                 ▼                                           │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                     External Managed Services                        │   │
│  │                                                                      │   │
│  │   ┌──────────────────┐   ┌──────────────────┐   ┌──────────────────┐│   │
│  │   │  PostgreSQL      │   │  MySQL           │   │  Redis           ││   │
│  │   │  (Temporal용)    │   │  (애플리케이션)  │   │  (캐시/락)       ││   │
│  │   │                  │   │                  │   │                  ││   │
│  │   │  • AWS RDS       │   │  • AWS RDS       │   │  • AWS ElastiC.  ││   │
│  │   │  • Cloud SQL     │   │  • Cloud SQL     │   │  • Redis Cluster ││   │
│  │   │  • Self-hosted   │   │  • Self-hosted   │   │  • Self-hosted   ││   │
│  │   │    (HA 구성)     │   │    (HA 구성)     │   │    (Sentinel)    ││   │
│  │   └──────────────────┘   └──────────────────┘   └──────────────────┘│   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  프로덕션 권장사항:                                                         │
│  • DB는 외부 관리형 서비스 사용 (RDS, Cloud SQL)                           │
│  • 또는 별도 DB 클러스터 구성 (Primary-Replica)                            │
│  • Temporal Server 서비스 분리 (frontend, history, matching, worker)       │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 4. Temporal Server 아키텍처 심화

### 4.1 Temporal Server 4대 서비스

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Temporal Server = 4개의 독립적으로 확장 가능한 서비스                      │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  1. Frontend Service                                                        │
│  ═══════════════════                                                        │
│  • 역할: gRPC API 게이트웨이                                               │
│  • 기능: 인증, 인가, 유효성 검증, 라우팅, Rate Limiting                    │
│  • 특성: Stateless (수평 확장 용이)                                        │
│  • 리소스: 1.5-2 CPU, 4GB Memory                                           │
│                                                                             │
│  2. History Service                                                         │
│  ══════════════════                                                         │
│  • 역할: Workflow 상태 및 이벤트 히스토리 관리                              │
│  • 기능: 이벤트 저장, Workflow 상태 머신, Task 생성                        │
│  • 특성: Sharded (numHistoryShards로 분산)                                 │
│  • 리소스: 4 CPU, 6GB+ Memory (메모리 70% 이하 유지)                       │
│  • 병목: 가장 중요한 서비스, 성능 병목 발생 가능                           │
│                                                                             │
│  3. Matching Service                                                        │
│  ════════════════════                                                       │
│  • 역할: Task Queue 관리 및 Worker 매칭                                    │
│  • 기능: Task dispatch, Queue partitioning, Poller 관리                    │
│  • 특성: Sharded (Task Queue 단위로 분산)                                  │
│  • 리소스: 1 CPU, 2GB Memory                                               │
│                                                                             │
│  4. Worker Service (내부용)                                                 │
│  ══════════════════════════                                                 │
│  • 역할: Temporal 내부 시스템 워크플로우 실행                               │
│  • 기능: Archival, 삭제, Dead-letter 처리                                  │
│  • 특성: 가장 리소스 적게 사용                                             │
│  • 리소스: 0.5-1 CPU, 1GB Memory                                           │
│                                                                             │
│  ※ 우리 애플리케이션의 Worker (OrderWorkflow 실행)와 다름!                 │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.2 배포 방식 비교

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  방식 1: All-in-One (temporalio/auto-setup)                                │
│  ═══════════════════════════════════════════                                │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                    temporal (auto-setup)                             │   │
│  │                                                                      │   │
│  │   ┌──────────┐ ┌──────────┐ ┌──────────┐ ┌──────────┐              │   │
│  │   │ Frontend │ │ History  │ │ Matching │ │  Worker  │              │   │
│  │   └──────────┘ └──────────┘ └──────────┘ └──────────┘              │   │
│  │                                                                      │   │
│  │   + 자동 스키마 설정                                                 │   │
│  │   + 내장 DB 가능 (SQLite)                                           │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  장점: 설정 간단, 빠른 시작                                                 │
│  단점: 개별 서비스 확장 불가, 프로덕션 비권장                               │
│  용도: 개발, 테스트, PoC                                                   │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  방식 2: 서비스 분리 (temporalio/server)                                   │
│  ═══════════════════════════════════════                                    │
│                                                                             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐   │
│  │  Frontend    │  │   History    │  │   Matching   │  │    Worker    │   │
│  │  replicas: 2 │  │  replicas: 3 │  │  replicas: 2 │  │  replicas: 1 │   │
│  └──────────────┘  └──────────────┘  └──────────────┘  └──────────────┘   │
│                                                                             │
│  장점: 개별 확장 가능, 세밀한 리소스 할당                                   │
│  단점: 설정 복잡, 스키마 수동 관리 필요                                     │
│  용도: 프로덕션                                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 4.3 History Shards 설정

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  numHistoryShards 설정 (매우 중요!)                                        │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ⚠️ 이 값은 클러스터 생성 후 변경 불가능!                                  │
│                                                                             │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Workflow 분산 방식                                                  │   │
│  │                                                                      │   │
│  │  WorkflowID → Hash → Shard 번호 → 담당 History Service              │   │
│  │                                                                      │   │
│  │  예: numHistoryShards=512, History 노드 4대                         │   │
│  │      → 각 노드가 128개 Shard 담당                                    │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  권장값:                                                                    │
│  ┌──────────────────┬──────────────────────────────────────────────────┐   │
│  │ 규모             │ numHistoryShards                                 │   │
│  ├──────────────────┼──────────────────────────────────────────────────┤   │
│  │ 소규모/테스트    │ 4 ~ 16                                           │   │
│  │ 중소규모         │ 128 ~ 512                                        │   │
│  │ 대규모           │ 512 ~ 4096                                       │   │
│  │ 초대규모         │ 4096 ~ 16384                                     │   │
│  └──────────────────┴──────────────────────────────────────────────────┘   │
│                                                                             │
│  계산 공식: History 노드 수 × (64 ~ 256)                                   │
│  예: 4대 History 노드 계획 → 4 × 128 = 512 shards                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 참고 자료

- [Temporal Server Architecture](https://docs.temporal.io/temporal-service/temporal-server)
- [Scaling Temporal: The basics](https://temporal.io/blog/scaling-temporal-the-basics)
- [Temporal Architecture Breakdown](https://medium.com/data-science-collective/system-design-series-a-step-by-step-breakdown-of-temporals-internal-architecture-52340cc36f30)

---

## 5. Docker Swarm 네트워킹

### 5.1 Overlay Network

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Overlay Network: 다중 노드 간 통신                                         │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   Node 1                                    Node 2                          │
│   ┌─────────────────────────────┐          ┌─────────────────────────────┐ │
│   │                             │          │                             │ │
│   │  ┌─────────┐ ┌─────────┐   │          │  ┌─────────┐ ┌─────────┐   │ │
│   │  │ order.1 │ │ order.2 │   │          │  │ order.3 │ │ inv.1   │   │ │
│   │  │10.0.1.2 │ │10.0.1.3 │   │          │  │10.0.1.4 │ │10.0.1.5 │   │ │
│   │  └────┬────┘ └────┬────┘   │          │  └────┬────┘ └────┬────┘   │ │
│   │       │           │        │          │       │           │        │ │
│   │       └─────┬─────┘        │          │       └─────┬─────┘        │ │
│   │             │              │          │             │              │ │
│   │   ┌─────────┴─────────┐    │          │   ┌─────────┴─────────┐    │ │
│   │   │ overlay: app-net  │    │──────────│   │ overlay: app-net  │    │ │
│   │   │ 10.0.1.0/24       │    │  VXLAN   │   │ 10.0.1.0/24       │    │ │
│   │   └───────────────────┘    │ (터널링) │   └───────────────────┘    │ │
│   │                             │          │                             │ │
│   └─────────────────────────────┘          └─────────────────────────────┘ │
│                                                                             │
│  특징:                                                                      │
│  • 모든 노드의 컨테이너가 같은 네트워크에 있는 것처럼 통신                  │
│  • VXLAN으로 캡슐화하여 물리 네트워크 위에서 동작                           │
│  • 암호화 옵션 지원 (--opt encrypted)                                      │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.2 서비스 디스커버리 & 로드 밸런싱

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Swarm 내장 DNS + VIP 로드 밸런싱                                           │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  orchestrator-temporal                                                      │
│      │                                                                      │
│      │ HTTP: http://service-order:21082/api/orders                         │
│      │                                                                      │
│      ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  Swarm DNS                                                           │   │
│  │                                                                      │   │
│  │  service-order → VIP: 10.0.1.100                                    │   │
│  │                                                                      │   │
│  │  (서비스 이름을 Virtual IP로 해석)                                   │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│      │                                                                      │
│      ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │  IPVS Load Balancer (L4)                                            │   │
│  │                                                                      │   │
│  │  VIP 10.0.1.100:21082                                               │   │
│  │      │                                                               │   │
│  │      ├───► 10.0.1.2:21082  (order.1) ─── Round Robin               │   │
│  │      ├───► 10.0.1.3:21082  (order.2)                                │   │
│  │      └───► 10.0.1.4:21082  (order.3)                                │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  ※ 코드 변경 없이 서비스 이름으로 호출하면 자동 로드 밸런싱                │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 5.3 Routing Mesh (Ingress)

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Routing Mesh: 외부 트래픽 분산                                             │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  External Client                                                            │
│      │                                                                      │
│      │ http://any-node-ip:21081/api/orders                                 │
│      │ (어떤 노드 IP로 접근해도 동작!)                                      │
│      ▼                                                                      │
│  ┌─────────────────────────────────────────────────────────────────────┐   │
│  │                                                                      │   │
│  │   Node 1 (:21081)      Node 2 (:21081)      Node 3 (:21081)         │   │
│  │        │                    │                    │                   │   │
│  │        └────────────────────┼────────────────────┘                   │   │
│  │                             │                                        │   │
│  │                             ▼                                        │   │
│  │                    ┌─────────────────┐                               │   │
│  │                    │  Ingress Load   │                               │   │
│  │                    │    Balancer     │                               │   │
│  │                    └────────┬────────┘                               │   │
│  │                             │                                        │   │
│  │              ┌──────────────┼──────────────┐                         │   │
│  │              ▼              ▼              ▼                         │   │
│  │        orchestrator.1  orchestrator.2  orchestrator.3               │   │
│  │        (Node 1)        (Node 2)        (Node 3)                     │   │
│  │                                                                      │   │
│  └─────────────────────────────────────────────────────────────────────┘   │
│                                                                             │
│  설정: ports 섹션에서 mode: ingress (기본값)                               │
│                                                                             │
│  ports:                                                                     │
│    - target: 21081                                                          │
│      published: 21081                                                       │
│      protocol: tcp                                                          │
│      mode: ingress       # 모든 노드에서 접근 가능                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 참고 자료

- [Manage swarm service networks](https://docs.docker.com/engine/swarm/networking/)
- [Use Swarm mode routing mesh](https://docs.docker.com/engine/swarm/ingress/)
- [HAProxy on Docker Swarm](https://www.haproxy.com/blog/haproxy-on-docker-swarm-load-balancing-and-dns-service-discovery)

---

## 6. 실습: Docker Compose → Swarm 스택

### 6.1 단일 노드 Swarm 스택 (개발용)

```yaml
# docker-stack.dev.yml
# 용도: 로컬 개발, 단일 노드 테스트

version: '3.8'

services:
  # ═══════════════════════════════════════════════════════════════════════════
  # 공유 인프라
  # ═══════════════════════════════════════════════════════════════════════════
  mysql:
    image: mysql:8.0
    environment:
      MYSQL_ROOT_PASSWORD: root1234
      TZ: Asia/Seoul
    ports:
      - "21306:3306"
    volumes:
      - mysql-data:/var/lib/mysql
    command:
      - --character-set-server=utf8mb4
      - --collation-server=utf8mb4_unicode_ci
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  redis:
    image: redis:7-alpine
    ports:
      - "21379:6379"
    volumes:
      - redis-data:/data
    command: redis-server --appendonly yes
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  # ═══════════════════════════════════════════════════════════════════════════
  # Temporal Infrastructure
  # ═══════════════════════════════════════════════════════════════════════════
  temporal-postgresql:
    image: postgres:15-alpine
    environment:
      POSTGRES_USER: temporal
      POSTGRES_PASSWORD: temporal
    ports:
      - "21432:5432"
    volumes:
      - temporal-postgresql-data:/var/lib/postgresql/data
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U temporal"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network

  temporal:
    image: temporalio/auto-setup:1.25.2
    ports:
      - "21733:7233"
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_SEEDS=temporal-postgresql
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "cluster", "health"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    networks:
      - app-network

  temporal-ui:
    image: temporalio/ui:2.31.2
    ports:
      - "21088:8080"
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000
    deploy:
      replicas: 1
    networks:
      - app-network

  # ═══════════════════════════════════════════════════════════════════════════
  # Application Services
  # ═══════════════════════════════════════════════════════════════════════════
  service-order:
    image: ${REGISTRY:-localhost:5000}/service-order:${TAG:-latest}
    environment:
      SPRING_PROFILES_ACTIVE: swarm
      SPRING_DATASOURCE_URL: jdbc:mysql://mysql:3306/orders?useSSL=false&allowPublicKeyRetrieval=true
      SPRING_DATASOURCE_USERNAME: root
      SPRING_DATASOURCE_PASSWORD: root1234
      SPRING_DATA_REDIS_HOST: redis
      SPRING_DATA_REDIS_PORT: 6379
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
      rollback_config:
        parallelism: 1
        delay: 10s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:21082/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - app-network

  service-inventory:
    image: ${REGISTRY:-localhost:5000}/service-inventory:${TAG:-latest}
    environment:
      SPRING_PROFILES_ACTIVE: swarm
      SPRING_DATASOURCE_URL: jdbc:mysql://mysql:3306/inventory?useSSL=false&allowPublicKeyRetrieval=true
      SPRING_DATASOURCE_USERNAME: root
      SPRING_DATASOURCE_PASSWORD: root1234
      SPRING_DATA_REDIS_HOST: redis
      SPRING_DATA_REDIS_PORT: 6379
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:21083/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - app-network

  service-payment:
    image: ${REGISTRY:-localhost:5000}/service-payment:${TAG:-latest}
    environment:
      SPRING_PROFILES_ACTIVE: swarm
      SPRING_DATASOURCE_URL: jdbc:mysql://mysql:3306/payments?useSSL=false&allowPublicKeyRetrieval=true
      SPRING_DATASOURCE_USERNAME: root
      SPRING_DATASOURCE_PASSWORD: root1234
      SPRING_DATA_REDIS_HOST: redis
      SPRING_DATA_REDIS_PORT: 6379
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:21084/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - app-network

  orchestrator-temporal:
    image: ${REGISTRY:-localhost:5000}/orchestrator-temporal:${TAG:-latest}
    ports:
      - "21081:21081"
    environment:
      SPRING_PROFILES_ACTIVE: swarm
      TEMPORAL_SERVICE_URL: temporal:7233
      # Swarm 내부 DNS로 서비스 호출!
      SERVICES_ORDER_URL: http://service-order:21082
      SERVICES_INVENTORY_URL: http://service-inventory:21083
      SERVICES_PAYMENT_URL: http://service-payment:21084
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 10s
        failure_action: rollback
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:21081/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - app-network

networks:
  app-network:
    driver: overlay
    attachable: true

volumes:
  mysql-data:
  redis-data:
  temporal-postgresql-data:
```

### 6.2 프로덕션 Swarm 스택 (Temporal 서비스 분리)

```yaml
# docker-stack.prod.yml
# 용도: 프로덕션, 3노드 이상 클러스터

version: '3.8'

services:
  # ═══════════════════════════════════════════════════════════════════════════
  # Temporal Server - 서비스 분리 배포
  # ═══════════════════════════════════════════════════════════════════════════

  # 공통 환경 변수는 x-temporal-common으로 재사용
  x-temporal-common: &temporal-common
    image: temporalio/server:1.25.2
    environment: &temporal-env
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=${TEMPORAL_DB_PASSWORD}
      - POSTGRES_SEEDS=${TEMPORAL_DB_HOST}
      - DYNAMIC_CONFIG_FILE_PATH=/etc/temporal/config/dynamicconfig/production.yaml
      - ENABLE_ES=false
    networks:
      - temporal-network
    secrets:
      - temporal_db_password

  temporal-frontend:
    <<: *temporal-common
    environment:
      <<: *temporal-env
      - SERVICES=frontend
      - FRONTEND_MEMBERSHIP_PORT=6933
      - FRONTEND_GRPC_PORT=7233
    ports:
      - "21733:7233"
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: start-first
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "tctl", "--address", "localhost:7233", "cluster", "health"]
      interval: 10s
      timeout: 5s
      retries: 10

  temporal-history:
    <<: *temporal-common
    environment:
      <<: *temporal-env
      - SERVICES=history
      - HISTORY_MEMBERSHIP_PORT=6934
      - HISTORY_GRPC_PORT=7234
      - NUM_HISTORY_SHARDS=512
    deploy:
      replicas: 3
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        order: stop-first
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  temporal-matching:
    <<: *temporal-common
    environment:
      <<: *temporal-env
      - SERVICES=matching
      - MATCHING_MEMBERSHIP_PORT=6935
      - MATCHING_GRPC_PORT=7235
    deploy:
      replicas: 2
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  temporal-worker:
    <<: *temporal-common
    environment:
      <<: *temporal-env
      - SERVICES=worker
      - WORKER_MEMBERSHIP_PORT=6936
      - WORKER_GRPC_PORT=7236
    deploy:
      replicas: 1
      resources:
        limits:
          cpus: '1'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  temporal-ui:
    image: temporalio/ui:2.31.2
    ports:
      - "21088:8080"
    environment:
      - TEMPORAL_ADDRESS=temporal-frontend:7233
    deploy:
      replicas: 1
    networks:
      - temporal-network

  # ═══════════════════════════════════════════════════════════════════════════
  # Application Services (생략 - 위 dev 버전과 유사하되 replicas 조정)
  # ═══════════════════════════════════════════════════════════════════════════

networks:
  temporal-network:
    driver: overlay
    attachable: true
  app-network:
    driver: overlay
    attachable: true

secrets:
  temporal_db_password:
    external: true
  mysql_password:
    external: true

volumes:
  mysql-data:
  redis-data:
```

---

## 7. Swarm 운영 명령어

### 7.1 클러스터 초기화 및 노드 관리

```bash
# ═══════════════════════════════════════════════════════════════════════════
# 1. Swarm 초기화 (첫 번째 Manager 노드에서)
# ═══════════════════════════════════════════════════════════════════════════

# 단일 노드 (로컬 개발)
docker swarm init

# 다중 노드 (advertise-addr 지정)
docker swarm init --advertise-addr 192.168.1.10

# ═══════════════════════════════════════════════════════════════════════════
# 2. 노드 추가
# ═══════════════════════════════════════════════════════════════════════════

# Worker 추가용 토큰 확인
docker swarm join-token worker

# Manager 추가용 토큰 확인
docker swarm join-token manager

# 다른 노드에서 실행 (Worker로 참여)
docker swarm join --token SWMTKN-xxx 192.168.1.10:2377

# 다른 노드에서 실행 (Manager로 참여)
docker swarm join --token SWMTKN-xxx 192.168.1.10:2377

# ═══════════════════════════════════════════════════════════════════════════
# 3. 노드 상태 확인
# ═══════════════════════════════════════════════════════════════════════════

# 노드 목록
docker node ls

# 노드 상세 정보
docker node inspect <node-id>

# 노드를 drain 모드로 (유지보수 시)
docker node update --availability drain <node-id>

# 노드 활성화
docker node update --availability active <node-id>

# 노드 제거
docker node rm <node-id>

# ═══════════════════════════════════════════════════════════════════════════
# 4. 레이블 관리
# ═══════════════════════════════════════════════════════════════════════════

# 노드에 레이블 추가 (배치 제약에 사용)
docker node update --label-add type=db node1
docker node update --label-add type=app node2
docker node update --label-add type=app node3

# 레이블 확인
docker node inspect --format '{{.Spec.Labels}}' node1
```

### 7.2 스택 배포 및 관리

```bash
# ═══════════════════════════════════════════════════════════════════════════
# 1. 스택 배포
# ═══════════════════════════════════════════════════════════════════════════

# 스택 배포
docker stack deploy -c docker-stack.dev.yml myapp

# 여러 compose 파일 합성
docker stack deploy -c docker-stack.yml -c docker-stack.override.yml myapp

# 환경 변수와 함께 배포
REGISTRY=myregistry.com TAG=v1.2.3 docker stack deploy -c docker-stack.yml myapp

# ═══════════════════════════════════════════════════════════════════════════
# 2. 스택 상태 확인
# ═══════════════════════════════════════════════════════════════════════════

# 스택 목록
docker stack ls

# 스택의 서비스 목록
docker stack services myapp

# 스택의 모든 태스크(컨테이너)
docker stack ps myapp

# 실패한 태스크만 보기
docker stack ps myapp --filter "desired-state=shutdown"

# ═══════════════════════════════════════════════════════════════════════════
# 3. 스택 제거
# ═══════════════════════════════════════════════════════════════════════════

# 스택 제거 (볼륨은 유지)
docker stack rm myapp

# 볼륨까지 제거하려면 별도로
docker volume rm myapp_mysql-data myapp_redis-data
```

### 7.3 서비스 관리

```bash
# ═══════════════════════════════════════════════════════════════════════════
# 1. 서비스 확인
# ═══════════════════════════════════════════════════════════════════════════

# 서비스 목록
docker service ls

# 서비스 상세 정보
docker service inspect myapp_service-order

# 서비스의 태스크(컨테이너) 목록
docker service ps myapp_service-order

# 서비스 로그
docker service logs myapp_service-order
docker service logs -f --tail 100 myapp_service-order

# ═══════════════════════════════════════════════════════════════════════════
# 2. 서비스 스케일링
# ═══════════════════════════════════════════════════════════════════════════

# 단일 서비스 스케일
docker service scale myapp_service-order=5

# 여러 서비스 동시 스케일
docker service scale myapp_service-order=5 myapp_service-inventory=3

# ═══════════════════════════════════════════════════════════════════════════
# 3. 서비스 업데이트 (롤링)
# ═══════════════════════════════════════════════════════════════════════════

# 이미지 업데이트
docker service update --image myregistry/service-order:v2.0.0 myapp_service-order

# 환경 변수 변경
docker service update --env-add NEW_VAR=value myapp_service-order

# 롤백
docker service rollback myapp_service-order

# 강제 재배포 (이미지 동일해도)
docker service update --force myapp_service-order
```

### 7.4 시크릿 및 설정 관리

```bash
# ═══════════════════════════════════════════════════════════════════════════
# 1. 시크릿 관리
# ═══════════════════════════════════════════════════════════════════════════

# 시크릿 생성 (파일에서)
echo "mypassword" | docker secret create db_password -

# 시크릿 생성 (파일에서)
docker secret create db_password ./password.txt

# 시크릿 목록
docker secret ls

# 시크릿 삭제
docker secret rm db_password

# ═══════════════════════════════════════════════════════════════════════════
# 2. 설정 관리
# ═══════════════════════════════════════════════════════════════════════════

# 설정 생성
docker config create app_config ./application.yml

# 설정 목록
docker config ls

# 설정 내용 확인
docker config inspect --pretty app_config
```

### 참고 자료

- [Deploy a stack to a swarm](https://docs.docker.com/engine/swarm/stack-deploy/)
- [Apply rolling updates](https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/)
- [Manage sensitive data with Docker secrets](https://docs.docker.com/engine/swarm/secrets/)

---

## 8. 롤링 업데이트 & 롤백

### 8.1 업데이트 전략

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Rolling Update 설정                                                        │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  deploy:                                                                    │
│    replicas: 4                                                              │
│    update_config:                                                           │
│      parallelism: 1          # 동시 업데이트 수                             │
│      delay: 10s              # 각 업데이트 사이 대기                        │
│      failure_action: rollback # 실패 시: rollback | pause | continue       │
│      monitor: 60s            # 업데이트 후 모니터링 시간                    │
│      max_failure_ratio: 0.2  # 허용 실패 비율 (20%)                         │
│      order: stop-first       # stop-first | start-first                    │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  order 옵션:                                                                │
│                                                                             │
│  stop-first (기본값)         start-first                                   │
│  ════════════════════         ════════════════════                          │
│                                                                             │
│  1. 기존 컨테이너 종료        1. 새 컨테이너 시작 (헬스체크 대기)           │
│  2. 새 컨테이너 시작          2. 헬스체크 통과 후                           │
│  3. 헬스체크 통과 대기        3. 기존 컨테이너 종료                         │
│                                                                             │
│  장점: 리소스 절약           장점: 무중단 (Zero Downtime)                  │
│  단점: 일시적 용량 감소      단점: 일시적 리소스 2배 필요                  │
│                                                                             │
│  ※ Stateless 서비스는 start-first 권장                                     │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 8.2 롤링 업데이트 흐름

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Rolling Update (parallelism=1, delay=10s, order=start-first)              │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  초기 상태 (v1.0)                                                           │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐                           │
│  │ order.1 │ │ order.2 │ │ order.3 │ │ order.4 │                           │
│  │  v1.0   │ │  v1.0   │ │  v1.0   │ │  v1.0   │                           │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘                           │
│                                                                             │
│  ────────────────────────────────────────────────────────────────────────  │
│                                                                             │
│  Step 1: 새 컨테이너 시작 (order.1 대체)                                    │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐               │
│  │ order.1 │ │ order.2 │ │ order.3 │ │ order.4 │ │ order.1 │               │
│  │  v1.0   │ │  v1.0   │ │  v1.0   │ │  v1.0   │ │  v2.0   │ Starting...   │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘ └────┬────┘               │
│                                                       │                     │
│                                                  Health Check               │
│                                                                             │
│  Step 2: 헬스체크 통과 → 기존 order.1 종료                                  │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐                           │
│  │ order.1 │ │ order.2 │ │ order.3 │ │ order.4 │                           │
│  │  v2.0 ✓ │ │  v1.0   │ │  v1.0   │ │  v1.0   │                           │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘                           │
│                                                                             │
│  ──── delay: 10s ────                                                       │
│                                                                             │
│  Step 3-4: 반복...                                                          │
│                                                                             │
│  최종 상태 (v2.0)                                                           │
│  ┌─────────┐ ┌─────────┐ ┌─────────┐ ┌─────────┐                           │
│  │ order.1 │ │ order.2 │ │ order.3 │ │ order.4 │                           │
│  │  v2.0 ✓ │ │  v2.0 ✓ │ │  v2.0 ✓ │ │  v2.0 ✓ │                           │
│  └─────────┘ └─────────┘ └─────────┘ └─────────┘                           │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 8.3 자동 롤백 설정

```yaml
deploy:
  update_config:
    failure_action: rollback    # 실패 시 자동 롤백
    monitor: 60s                # 60초 동안 모니터링
    max_failure_ratio: 0.2      # 20% 이상 실패 시 롤백 트리거

  rollback_config:
    parallelism: 2              # 롤백 시 동시 처리 수
    delay: 5s                   # 롤백 사이 대기
    failure_action: pause       # 롤백도 실패 시 일시 정지
    monitor: 30s
    order: start-first          # 롤백은 빠르게 start-first
```

### 참고 자료

- [Rolling updates with Docker Swarm](https://blog.container-solutions.com/rolling-updates-with-docker-swarm)
- [Docker Swarm Rolling Updates and Rollbacks](https://www.geeksforgeeks.org/devops/docker-swarm-rolling-updates-rollbacks/)

---

## 9. Health Check 전략

### 9.1 다층 헬스체크

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Health Check Layers                                                        │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Layer 1: Docker Container Health Check                                     │
│  ═══════════════════════════════════════                                    │
│                                                                             │
│  healthcheck:                                                               │
│    test: ["CMD", "curl", "-f", "http://localhost:21082/actuator/health"]   │
│    interval: 30s        # 체크 간격                                         │
│    timeout: 10s         # 타임아웃                                          │
│    retries: 3           # 실패 허용 횟수                                    │
│    start_period: 60s    # 시작 후 안정화 대기 (이 시간 동안 실패 무시)     │
│                                                                             │
│  상태: starting → healthy → unhealthy                                       │
│  unhealthy 시: Swarm이 컨테이너 재시작                                     │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Layer 2: Spring Boot Actuator Health                                       │
│  ═════════════════════════════════════                                      │
│                                                                             │
│  GET /actuator/health                                                       │
│                                                                             │
│  {                                                                          │
│    "status": "UP",                                                          │
│    "components": {                                                          │
│      "db": { "status": "UP" },                                             │
│      "redis": { "status": "UP" },                                          │
│      "diskSpace": { "status": "UP" }                                       │
│    }                                                                        │
│  }                                                                          │
│                                                                             │
│  구성요소 중 하나라도 DOWN → 전체 DOWN                                      │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Layer 3: Swarm Service Health                                              │
│  ════════════════════════════════                                           │
│                                                                             │
│  • 컨테이너 헬스체크 기반 트래픽 라우팅                                     │
│  • healthy 컨테이너만 로드밸런서에 등록                                     │
│  • unhealthy 컨테이너 자동 제외                                             │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 9.2 서비스별 헬스체크 권장 설정

```yaml
# MSA 서비스 (Spring Boot)
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:${PORT}/actuator/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 60s    # Spring Boot 시작 시간 고려

# Temporal Server
healthcheck:
  test: ["CMD", "tctl", "--address", "localhost:7233", "cluster", "health"]
  interval: 10s
  timeout: 5s
  retries: 10
  start_period: 30s

# MySQL
healthcheck:
  test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
  interval: 10s
  timeout: 5s
  retries: 5

# Redis
healthcheck:
  test: ["CMD", "redis-cli", "ping"]
  interval: 10s
  timeout: 5s
  retries: 5

# PostgreSQL
healthcheck:
  test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER}"]
  interval: 10s
  timeout: 5s
  retries: 5
```

---

## 10. 모니터링 및 로깅

### 10.1 Swarm 통합 로깅

```yaml
# 서비스 정의에 로깅 드라이버 설정
services:
  service-order:
    image: myregistry/service-order:latest
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        labels: "service,environment"
        env: "SPRING_PROFILES_ACTIVE"

# 또는 중앙 집중식 로깅 (Loki, ELK)
services:
  service-order:
    logging:
      driver: "loki"
      options:
        loki-url: "http://loki:3100/loki/api/v1/push"
        loki-batch-size: "400"
```

### 10.2 모니터링 스택 (간단 버전)

```yaml
# 모니터링 서비스 추가
services:
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    deploy:
      replicas: 1
    networks:
      - monitoring

networks:
  monitoring:
    driver: overlay
```

---

## 11. 실습 가이드

### 11.1 로컬에서 Swarm 시작하기

```bash
# ═══════════════════════════════════════════════════════════════════════════
# Step 1: Swarm 초기화
# ═══════════════════════════════════════════════════════════════════════════
docker swarm init

# 출력:
# Swarm initialized: current node (xxx) is now a manager.
# To add a worker: docker swarm join --token SWMTKN-xxx ...

# ═══════════════════════════════════════════════════════════════════════════
# Step 2: 애플리케이션 이미지 빌드 및 태그
# ═══════════════════════════════════════════════════════════════════════════
cd spring-temporal-exam

# 각 서비스 빌드
./gradlew :service-order:bootJar
./gradlew :service-inventory:bootJar
./gradlew :service-payment:bootJar
./gradlew :orchestrator-temporal:bootJar

# Docker 이미지 빌드 (Dockerfile 필요)
docker build -t localhost:5000/service-order:latest ./service-order
docker build -t localhost:5000/service-inventory:latest ./service-inventory
docker build -t localhost:5000/service-payment:latest ./service-payment
docker build -t localhost:5000/orchestrator-temporal:latest ./orchestrator-temporal

# ═══════════════════════════════════════════════════════════════════════════
# Step 3: 스택 배포
# ═══════════════════════════════════════════════════════════════════════════
docker stack deploy -c docker-stack.dev.yml myapp

# ═══════════════════════════════════════════════════════════════════════════
# Step 4: 상태 확인
# ═══════════════════════════════════════════════════════════════════════════
docker stack services myapp
docker stack ps myapp

# ═══════════════════════════════════════════════════════════════════════════
# Step 5: 테스트
# ═══════════════════════════════════════════════════════════════════════════
curl -X POST http://localhost:21081/api/temporal/orders \
  -H "Content-Type: application/json" \
  -d '{"customerId": 1, "productId": 1, "quantity": 2, "amount": 20000}'

# ═══════════════════════════════════════════════════════════════════════════
# Step 6: 스케일 조정
# ═══════════════════════════════════════════════════════════════════════════
docker service scale myapp_service-order=3

# ═══════════════════════════════════════════════════════════════════════════
# Step 7: 정리
# ═══════════════════════════════════════════════════════════════════════════
docker stack rm myapp
docker swarm leave --force
```

### 11.2 application.yml 수정 (Swarm 프로파일)

```yaml
# application-swarm.yml
spring:
  config:
    activate:
      on-profile: swarm

  datasource:
    # 환경 변수로 오버라이드 가능
    url: ${SPRING_DATASOURCE_URL:jdbc:mysql://mysql:3306/orders}
    username: ${SPRING_DATASOURCE_USERNAME:root}
    password: ${SPRING_DATASOURCE_PASSWORD:root1234}

  data:
    redis:
      host: ${SPRING_DATA_REDIS_HOST:redis}
      port: ${SPRING_DATA_REDIS_PORT:6379}

# Temporal 연결 (Swarm 서비스 이름)
temporal:
  service:
    url: ${TEMPORAL_SERVICE_URL:temporal:7233}

# 다른 서비스 호출 URL (Swarm 내부 DNS)
services:
  order:
    url: ${SERVICES_ORDER_URL:http://service-order:21082}
  inventory:
    url: ${SERVICES_INVENTORY_URL:http://service-inventory:21083}
  payment:
    url: ${SERVICES_PAYMENT_URL:http://service-payment:21084}
```

---

## 12. 체크리스트

### 12.1 배포 전 체크리스트

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  Docker Swarm 배포 전 체크리스트                                            │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  Infrastructure                                                             │
│  ☐ Manager 노드 수 결정 (홀수: 1, 3, 5)                                    │
│  ☐ Worker 노드 수 결정                                                     │
│  ☐ 네트워크 구성 확인 (overlay 네트워크)                                   │
│  ☐ 볼륨 백업 전략 수립                                                     │
│  ☐ 시크릿 생성 완료                                                        │
│                                                                             │
│  Application                                                                │
│  ☐ Docker 이미지 빌드 및 푸시                                              │
│  ☐ 환경별 설정 파일 준비 (application-swarm.yml)                           │
│  ☐ 헬스체크 엔드포인트 구현 (/actuator/health)                             │
│  ☐ Graceful shutdown 설정                                                  │
│                                                                             │
│  Temporal                                                                   │
│  ☐ numHistoryShards 결정 (변경 불가!)                                      │
│  ☐ DB 스키마 마이그레이션                                                  │
│  ☐ Namespace 생성                                                          │
│                                                                             │
│  Deployment                                                                 │
│  ☐ replicas 수 결정                                                        │
│  ☐ update_config 설정                                                      │
│  ☐ rollback_config 설정                                                    │
│  ☐ 리소스 제한 설정 (resources.limits)                                     │
│  ☐ 배치 제약 조건 설정 (placement.constraints)                             │
│                                                                             │
│  Monitoring                                                                 │
│  ☐ 로깅 드라이버 설정                                                      │
│  ☐ Prometheus/Grafana 구성                                                 │
│  ☐ 알림 설정                                                               │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 12.2 운영 체크리스트

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  일일 운영 체크리스트                                                       │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  ☐ docker node ls - 모든 노드 Ready 상태 확인                              │
│  ☐ docker service ls - 모든 서비스 replicas 정상 확인                      │
│  ☐ docker stack ps myapp - 실패한 태스크 없는지 확인                       │
│  ☐ 로그 이상 확인                                                          │
│  ☐ 디스크 사용량 확인                                                      │
│  ☐ Temporal UI에서 실패한 Workflow 확인                                    │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 13. 트러블슈팅

### 13.1 일반적인 문제

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  문제: 서비스가 시작되지 않음                                               │
│                                                                             │
│  진단:                                                                      │
│  docker service ps myapp_service-order --no-trunc                          │
│  docker service logs myapp_service-order                                   │
│                                                                             │
│  일반적인 원인:                                                             │
│  • 이미지 pull 실패 → 레지스트리 접근 확인                                 │
│  • 헬스체크 실패 → start_period 증가                                       │
│  • 리소스 부족 → 노드 리소스 확인                                          │
│  • 볼륨 마운트 실패 → 볼륨 경로 확인                                       │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  문제: 서비스 간 통신 실패                                                  │
│                                                                             │
│  진단:                                                                      │
│  docker exec -it <container-id> sh                                         │
│  nslookup service-order                                                    │
│  curl http://service-order:21082/actuator/health                           │
│                                                                             │
│  일반적인 원인:                                                             │
│  • 같은 네트워크에 없음 → networks 설정 확인                               │
│  • DNS 해석 실패 → 서비스 이름 오타 확인                                   │
│  • 포트 불일치 → 컨테이너 내부 포트 확인                                   │
│                                                                             │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│  문제: 롤링 업데이트 중 다운타임                                            │
│                                                                             │
│  해결:                                                                      │
│  • order: start-first 사용                                                 │
│  • 헬스체크 정확히 설정                                                    │
│  • replicas >= 2 유지                                                       │
│  • Spring Boot graceful shutdown 설정:                                     │
│    server.shutdown=graceful                                                │
│    spring.lifecycle.timeout-per-shutdown-phase=30s                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

---

## 14. 참고 자료

### 공식 문서

- [Docker Swarm Mode](https://docs.docker.com/engine/swarm/)
- [Deploy to Swarm](https://docs.docker.com/guides/swarm-deploy/)
- [Temporal Deployment Guide](https://docs.temporal.io/self-hosted-guide/deployment)
- [Temporal Helm Charts](https://github.com/temporalio/helm-charts)

### 커뮤니티

- [Temporal Community Forum](https://community.temporal.io/)
- [Docker Community Forums - Swarm](https://forums.docker.com/c/docker-engine/swarm/17)

### 추가 학습

- [HA Docker Swarm Setup](https://betterstack.com/community/guides/scaling-docker/ha-docker-swarm/)
- [Temporal Production HA Setup](https://community.temporal.io/t/production-ha-setup/225)
- [Mastering Self-Hosted Temporal Clusters](https://temporal.io/resources/on-demand/mastering-self-hosted-temporal-clusters)

---

*이전 문서: `01-github-actions.md`*
*관련 문서: `../phase3/04-temporal-msa-architecture-flow.md`*
